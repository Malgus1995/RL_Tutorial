{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "34fa128e-6296-4bb2-8dae-09bc63ba8586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "ba5aff94-0cbf-41de-8222-88ea8866f11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsp_data_txt = open('tsp_dataset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "98f2c30d-d9c5-4062-924e-8f1bce1db0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = tsp_data_txt.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "9ada7c7e-ce00-4546-9fc6-67a1621d81e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsp_data_txt = open('tsp_dataset.txt')\n",
    "data = {}\n",
    "lines = tsp_data_txt.readlines()\n",
    "distance_matrix = np.zeros((len(lines),len(lines)))\n",
    "for i,line in enumerate(lines):\n",
    "    for j,element in enumerate((line.split('\\n')[0].split('       '))):\n",
    "        if(element == ''):\n",
    "            continue\n",
    "        \n",
    "        distance_matrix[i][j-1] = int(element.strip())\n",
    "data['distance_matrix']= distance_matrix\n",
    "data['reward_matrix'] = [1 for i  in range(0,len(distance_matrix))]\n",
    "data['depot'] = 0\n",
    "data['n_state'] = len(distance_matrix)*len(distance_matrix)\n",
    "data['n_action'] = len(distance_matrix)\n",
    "data['visited_nodes'] = [0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "a01ad9d1-f6a4-43c7-85ae-1cfba05ae909",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(data['distance_matrix'])):\n",
    "    data['distance_matrix'][i][i] = 9999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "d07f3db5-eb8b-48e3-8e1b-043734f77f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialze_value(env):\n",
    "    initial_P = 1/(env['n_action']-1)\n",
    "    env['P'] = {}\n",
    "    \n",
    "    #P[state][action] = [(prob, next state, reward, done), ...]\n",
    "    \n",
    "    for i in range(0,env['n_state']):\n",
    "        env['P'][i] ={}\n",
    "        for j in range(0,env['n_action']):\n",
    "            env['P'][i][j] = []\n",
    "            for k in range(0,env['n_action']):\n",
    "                ini_temp = [initial_P,k,data['reward_matrix'][k],False]\n",
    "                env['P'][i][j].append(ini_temp)\n",
    "            \n",
    "    return env\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "465ac45c-e671-4f8f-ac37-60f99897255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_reward(envp):\n",
    "    for i in range(0,envp['n_action']):\n",
    "        for j in range(0,envp['n_action']):\n",
    "            for k in range(0,envp['n_action']):\n",
    "                if k==0:\n",
    "                    env['P'][i][j][k][2]= -9999\n",
    "                    env['P'][i][j][k][3]= True\n",
    "                    break\n",
    "    return envp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "cc3f3f63-947c-4988-b493-6a727ff7aadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = initialze_value(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "e709d6a4-1a07-416e-aee1-f86cfc41869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in env['P'].keys():\n",
    "    for a in env['P'][s]:\n",
    "        for s_i,s_p in enumerate(env['P'][s][a]):\n",
    "            if(s==a==s_i):\n",
    "                env['P'][s][a][s_i][2] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "9b8106bd-86ff-4428-b3ad-a3333019baac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9999.,   29.,   82.,   46.,   68.,   52.,   72.,   42.,   51.,\n",
       "         55.,   29.,   74.,   23.,   72.,   46.])"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env['distance_matrix'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "fe349189-9ea0-467c-ab8e-f6da57c52246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roots(v_list,env_dist=env['distance_matrix']):\n",
    "    v_count = 14\n",
    "    ini_point =0\n",
    "    distance = 0\n",
    "    for v_c in range(0,14):\n",
    "        i = v_list[ini_point]\n",
    "        j = v_list[ini_point+1]\n",
    "        distance+=env['distance_matrix'][i][j]\n",
    "        ini_point+=1\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a9e2a1-fc1d-42a3-9930-50ab65ecae35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "cc4122fa-02f5-416e-aa56-bf093b10ee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pi = np.random.uniform(size=(env['n_state'],env['n_action']))\n",
    "Pi = Pi/np.sum(Pi,axis=1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "488821db-e01f-48d9-befe-87e2b1ac99cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_policy_evaluation(env,P,r,Pi,gamma=0.99,epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Policy evaluation \n",
    "     env   : environment\n",
    "     P     : state transition probability [S x A x S]\n",
    "     r     : reward [S]\n",
    "     Pi    : policy [S x A]\n",
    "    returns\n",
    "     Q     : Q-value [S x A]\n",
    "    \"\"\"\n",
    "    # Extract environment information\n",
    "    n_state = env['n_state']\n",
    "    n_action = env['n_action']\n",
    "    # Randomly initialize Q\n",
    "    Q = np.random.uniform(size=(n_state,n_action))\n",
    "    qe_count = 0\n",
    "    while True:\n",
    "        V = np.sum(Pi*Q,axis=1) # [S]\n",
    "        \n",
    "        V_tile = np.tile(V[np.newaxis,np.newaxis:],reps=(n_action,n_action,1)) # [S x A x S]\n",
    "        V_tile = np.reshape(V_tile,(225,15,15))\n",
    "        Q_prime = np.sum((r+gamma*V_tile)*P,axis=2) # [S x A]\n",
    "        Q_dist = np.max(np.max(np.abs(Q-Q_prime)))\n",
    "        #print(np.where(np.max(np.max(np.abs(Q-Q_prime)))==Q_dist)[0][0])\n",
    "        Q = Q_prime\n",
    "        \n",
    "        #if(qe_count%10==0):\n",
    "            #print(Q_dist)\n",
    "        if(qe_count==100):\n",
    "            break\n",
    "        \n",
    "        qe_count+=1\n",
    "        \n",
    "        if Q_dist < epsilon:\n",
    "            break\n",
    "    return Q\n",
    "\n",
    "\n",
    "def q_policy_improvement(env,Q):\n",
    "    \"\"\"\n",
    "    Policy improvement\n",
    "     env   : environment\n",
    "     Q     : Q-value [S x A]\n",
    "    \"\"\"\n",
    "    # Extract environment information\n",
    "    n_state = env['n_state']\n",
    "    n_action = env['n_action']\n",
    "    Pi = np.zeros((n_state,n_action))\n",
    "    rm_lst = []\n",
    "    \n",
    "    while True:\n",
    "        count=0\n",
    "        for q_i,q_ in enumerate(Q):\n",
    "            argmax = np.argmax(q_)\n",
    "            if argmax not in rm_lst:\n",
    "                rm_lst.append(argmax)\n",
    "            if argmax in rm_lst:\n",
    "                if(count%5==0):\n",
    "                    Q[q_i][argmax] -= (1/env['distance_matrix'][q_i//15][argmax])*count\n",
    "                count+=1\n",
    "                #if count>20:\n",
    "                    #Q[q_i][argmax] = -100\n",
    "        if(len(rm_lst)==15):\n",
    "            break\n",
    "        rm_lst = []\n",
    "    Pi[np.arange(n_state),np.argmax(Q,axis=1)] = 1 # Greedy policy update\n",
    "    return Pi\n",
    "\n",
    "def q_policy_iteration(env,gamma=0.99,epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Policy iteration\n",
    "    \"\"\"\n",
    "    q_count =0 \n",
    "    n_state = env['n_state']\n",
    "    n_action = env['n_action']\n",
    "    # Random initial policy\n",
    "    Pi = np.random.uniform(size=(n_state,n_action))\n",
    "    Pi = Pi/np.sum(Pi,axis=1,keepdims=True)\n",
    "    # Parse P and r from the environment\n",
    "    P = np.zeros((n_state,n_action,n_action))\n",
    "    r = np.zeros((n_state,n_action,n_action))\n",
    "    for s in env['P'].keys(): # for all states s\n",
    "        for a in env['P'][s].keys(): # for all actions a\n",
    "            for prob,s_prime,reward,done in env['P'][s][a]:\n",
    "                P[s][a][s_prime] = prob # model \n",
    "                r[s][a][s_prime] = (1/env['distance_matrix'][s//15][a])*reward\n",
    "                #r[s][a][s_prime] = reward\n",
    "    # Loop\n",
    "    \n",
    "    while True:\n",
    "        Q = q_policy_evaluation(env,P,r,Pi,gamma=gamma,epsilon=epsilon) # policy evaluation\n",
    "        \n",
    "        Pi_prime = q_policy_improvement(env,Q) # policy improvement\n",
    "        \n",
    "        if (Pi == Pi_prime).all(): # if policy does not change\n",
    "            break\n",
    "        Pi = Pi_prime # update new policy\n",
    "        break\n",
    "    return Pi,Q,r,P\n",
    "    #return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "0ed40fdf-f31b-4b6b-8814-4d7401905ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pi,Q,r,p = q_policy_iteration(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "cbba9706-39c2-4f64-9e8e-4070a448513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAgent():\n",
    "    def __init__(self,n_state,n_action,epsilon=1.0,alpha=0.1,gamma=0.995):\n",
    "        \"\"\"\n",
    "        Initialize Monte Carlo Learning Agent \n",
    "        \"\"\"\n",
    "        self.n_state = n_state\n",
    "        self.n_action = n_action\n",
    "        self.epsilon = epsilon # epsilon greediness\n",
    "        self.alpha = alpha # Q mixing rate\n",
    "        self.gamma = gamma # discount factor\n",
    "        \n",
    "        # Initialize Q value\n",
    "        self.Q = np.zeros([n_state,n_action])\n",
    "        # Memory \n",
    "        self.samples = []\n",
    "        \n",
    "    def save_sample(self, state, action, reward, done): \n",
    "        \"\"\"\n",
    "        Save experience (s, a, r, done)\n",
    "        \"\"\"\n",
    "        self.samples.append([state, action, reward, done])\n",
    "        \n",
    "    def update_Q(self):\n",
    "        \"\"\"\n",
    "        Update Q\n",
    "        \"\"\"\n",
    "        Q_old = self.Q # [S x A]\n",
    "        g = 0\n",
    "        G = Q_old\n",
    "        for t in reversed(range(len(self.samples))): # for all samples in a reversed way\n",
    "            state,action,reward,_ = self.samples[t]\n",
    "            g = reward + self.gamma*g # g = r + gamma * g\n",
    "            G[state][action] = g # update G\n",
    "            \n",
    "        # Update Q\n",
    "        self.Q = Q_old + self.alpha*(G - Q_old)\n",
    "        # Empty memory\n",
    "        self.samples = []\n",
    "        \n",
    "    def update_epsilon(self,epsilon):\n",
    "        self.epsilon = np.min([epsilon,1.0]) # decay\n",
    "        \n",
    "    def get_action(self,state):\n",
    "        \"\"\"\n",
    "        Get action\n",
    "        \"\"\"\n",
    "        if np.random.uniform() < self.epsilon: # random with epsilon probability \n",
    "            \n",
    "            action = np.random.randint(0, high=self.n_action)\n",
    "        else: # greedy action\n",
    "            action = np.argmax(self.Q[state])\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "c9dcc03c-5e20-4170-a042-7b1a7defe3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = initialze_value(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "e40f63ca-2ab8-4857-aa16-8cbefdcd1c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MC learning done.\n"
     ]
    }
   ],
   "source": [
    "n_state = env['n_state']\n",
    "n_action = env['n_action']\n",
    "M = MCAgent(n_state,n_action,epsilon=1,alpha=0.1,gamma=0.999)\n",
    "envp = env['P']\n",
    "n_episode = 1000\n",
    "\n",
    "for e_idx in range(n_episode):\n",
    "    rm_lst=[0]\n",
    "    state = 0\n",
    "    action = M.get_action(state)\n",
    "    done = False\n",
    "    \n",
    "    \n",
    "    while not done:\n",
    "        if(len(rm_lst)==15):\n",
    "            break\n",
    "            \n",
    "        if action not in rm_lst:\n",
    "            rm_lst.append(action)\n",
    "    \n",
    "        next_state = action\n",
    "        #print(state,action,state+action,envp[state][action][(state+action)//15])\n",
    "        if action in rm_lst:\n",
    "            reward=-100\n",
    "            if e_idx>0:\n",
    "                e_idx-=1\n",
    "                \n",
    "        next_action = M.get_action(next_state) # Get next action\n",
    "        M.save_sample(state,action,reward,done) # Store samples\n",
    "        state = next_state\n",
    "        action = next_action\n",
    " \n",
    "        \n",
    "    # End of the episode\n",
    "    M.update_Q() # Update Q value using sampled epsiode\n",
    "    M.update_epsilon(100/(e_idx+1)) # Decaying epsilon\n",
    "print (\"MC learning done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdebe81d-3356-4334-9a56-cdc2136d8a85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "9dbcd88a-4627-49be-a904-2a833880f1a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.get_action(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "30067a63-233c-4d84-9aef-a64fee49bba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "d84a72fe-7079-44c3-800b-5b6c89f3688b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20889436331238465"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "74874936-e719-44d4-972a-07bfd7e60382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 5, 11, 1, 8, 3, 10, 13, 12, 14, 7, 9, 6, 4, 2]"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ee45ab34-d35d-4bbc-8985-0ed91d46b569",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImportanceSampling(MCAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(ImportanceSampling, self).__init__(*args, **kwargs)\n",
    "        self.C = np.zeros([self.n_state,self.n_action])\n",
    "        self.b = self.set_behavior_policy(p_type='random')\n",
    "        \n",
    "    def set_behavior_policy(self, p_type):\n",
    "        \"\"\"\n",
    "        Set behavior policy\n",
    "        \"\"\"\n",
    "        if p_type == \"random\":\n",
    "            def behavior_policy(state):\n",
    "                prob = np.ones(self.n_action) / self.n_action\n",
    "                return prob\n",
    "        elif p_type == \"greedy\":\n",
    "            def behavior_policy(state):\n",
    "                prob = np.zeros(self.n_action)\n",
    "                prob[np.argmax(state)] =1\n",
    "                return prob\n",
    "        return behavior_policy\n",
    "            \n",
    "            \n",
    "    def update_Q(self):\n",
    "        \"\"\"\n",
    "        Update Q\n",
    "        \"\"\"\n",
    "        g = 0.\n",
    "        for t in reversed(range(len(self.samples))): # for all samples in a reversed way\n",
    "            state,action,reward,_ = self.samples[t]\n",
    "            g = (reward + self.gamma*g) # g = (r + gamma * g) * 1/b\n",
    "            self.C[state][action] += 1\n",
    "            self.Q[state][action] += (1 / self.C[state][action]) * (g - self.Q[state][action])\n",
    "            if action !=  np.argmax(self.Q[state]):\n",
    "                break\n",
    "            \n",
    "        # Empty memory\n",
    "        self.samples = []\n",
    "                    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Get action\n",
    "        \"\"\"\n",
    "        probs = self.b(state)\n",
    "        action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "c5f2edd2-67db-4b9a-bb5d-9936e1ed7513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MC learning done.\n"
     ]
    }
   ],
   "source": [
    "n_state = env['n_state']\n",
    "n_action = env['n_action']\n",
    "M = ImportanceSampling(n_state,n_action,epsilon=1.0,alpha=0.1,gamma=0.999)\n",
    "\n",
    "envp = env['P']\n",
    "n_episode = 1000\n",
    "\n",
    "for e_idx in range(n_episode):\n",
    "    rm_lst=[0]\n",
    "    state = 0\n",
    "    action = M.get_action(state)\n",
    "    done = False\n",
    "    \n",
    "    \n",
    "    while not done:\n",
    "        if(len(rm_lst)==15):\n",
    "            break\n",
    "            \n",
    "        if action not in rm_lst:\n",
    "            rm_lst.append(action)\n",
    "    \n",
    "        next_state = action\n",
    "        #print(state,action,state+action,envp[state][action][(state+action)//15])\n",
    "        if action in rm_lst:\n",
    "            reward=-100\n",
    "            if e_idx>0:\n",
    "                e_idx-=1\n",
    "                \n",
    "        next_action = M.get_action(next_state) # Get next action\n",
    "        M.save_sample(state,action,reward,done) # Store samples\n",
    "        state = next_state\n",
    "        action = next_action\n",
    " \n",
    "        \n",
    "    # End of the episode\n",
    "    M.update_Q() # Update Q value using sampled epsiode\n",
    "    M.update_epsilon(100/(e_idx+1)) # Decaying epsilon\n",
    "print (\"MC learning done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "39e29ab9-4e0b-4635-809a-d6ce3b95853a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 9, 4, 5, 7, 8, 3, 2, 12, 14, 10, 13, 6, 1, 11]"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "5450810e-c1dd-48e2-ac25-e9f97146f857",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_8172/525519296.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_8172/525519296.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    |\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "3f3fd8fb-6e58-4061-b6a8-b1bd3d59cd8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebddca1-cb33-40d2-9a65-30f9eaf11623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
