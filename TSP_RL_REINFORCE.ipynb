{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a148b7d-0892-4288-a452-efd3823b314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bc71f55-2ad8-4d7b-a4f6-1f4760fd3c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsp_data_txt = open('tsp_dataset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14ec35f4-fd71-46a3-a8ef-b2b62abcb03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = tsp_data_txt.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cda82ea3-8e67-4ce2-b5fe-e1a38d42d630",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsp_data_txt = open('tsp_dataset.txt')\n",
    "data = {}\n",
    "lines = tsp_data_txt.readlines()\n",
    "distance_matrix = np.zeros((len(lines),len(lines)))\n",
    "for i,line in enumerate(lines):\n",
    "    for j,element in enumerate((line.split('\\n')[0].split('       '))):\n",
    "        if(element == ''):\n",
    "            continue\n",
    "        \n",
    "        distance_matrix[i][j-1] = int(element.strip())\n",
    "data['distance_matrix']= distance_matrix\n",
    "data['reward_matrix'] = [1 for i  in range(0,len(distance_matrix))]\n",
    "data['depot'] = 0\n",
    "data['n_state'] = sum(list(range(1,len(distance_matrix))))\n",
    "data['n_action'] = len(distance_matrix)\n",
    "data['visited_nodes'] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6ef24ca-4337-4e66-9b91-57202fab28be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(data['distance_matrix'])):\n",
    "    data['distance_matrix'][i][i] = 9999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e73bbd8e-494b-4d26-88ea-d8879e4d9d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_matrix[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c81d52a-4038-462f-ace1-95816618e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_reward(envp):\n",
    "    for i in range(0,envp['n_state']):\n",
    "        for j in range(0,envp['n_action']):\n",
    "            for k in range(0,envp['n_action']):\n",
    "                if k==0:\n",
    "                    env['P'][i][j][k][2]= 10\n",
    "                    env['P'][i][j][k][3]= True\n",
    "                    break\n",
    "    return envp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73d7d7df-ee9c-496a-ace9-d06907817439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialze_value(env):\n",
    "    initial_P = 1/(env['n_action']-1)\n",
    "    env['P'] = {}\n",
    "    \n",
    "    #P[state][action] = [(prob, next state, reward, done), ...]\n",
    "    \n",
    "    for i in range(0,env['n_state']):\n",
    "        env['P'][i] ={}\n",
    "        \n",
    "        if(i==104):\n",
    "            for j in range(0,env['n_action']):\n",
    "                env['P'][i][j] = []\n",
    "                initial_P = 1/(env['n_action']-1)\n",
    "                for k in range(0,env['n_action']):\n",
    "                    if (j == env['depot'] and k == env['depot']):\n",
    "                        ini_temp = [1,k,10,True]\n",
    "                        env['P'][i][j].append(ini_temp)\n",
    "                        initial_P = 0\n",
    "                        continue\n",
    "                    ini_temp = [initial_P,k,data['reward_matrix'][k],False]\n",
    "                    env['P'][i][j].append(ini_temp)\n",
    "        else:\n",
    "            for j in range(0,env['n_action']):\n",
    "                env['P'][i][j] = []\n",
    "                for k in range(0,env['n_action']):\n",
    "                    ini_temp = [initial_P,k,data['reward_matrix'][k],False]\n",
    "                    env['P'][i][j].append(ini_temp)\n",
    "    \n",
    "    env['state'] = []\n",
    "    for si in range(0,env['n_state']):\n",
    "        env['state'].append([])\n",
    "        for n in range(0,env['n_action']):\n",
    "            env['state'][si].append(initial_P)\n",
    "    \n",
    "    env['state'] = np.asarray(env['state']).reshape((env['n_action'],-1))\n",
    "            \n",
    "    return env\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5b6c111-0f52-4bc0-8384-8487a4b03814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.0002\n",
    "gamma         = 0.98\n",
    "\n",
    "# Run configurations\n",
    "print_every = 100\n",
    "num_episodes = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8fad7cb-44cd-4dbd-825a-36e410df14a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(REINFORCE, self).__init__()\n",
    "        self.linear_1 = nn.Linear(105, 128)\n",
    "        self.linear_2 = nn.Linear(128, 15)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.data = []\n",
    "        self.visited_list =[]\n",
    "    \n",
    "    def get_action_with_softmax(self,prob):\n",
    "        visited_index = []\n",
    "        prob[0][0] =  torch.tensor(float('-inf'))\n",
    "        first_index = Categorical(F.softmax(prob[0])).sample()\n",
    "        visited_index.append(first_index)\n",
    "        for pi,pb in enumerate(prob[1:]):    \n",
    "            if pi <13:\n",
    "                prob[pi+1][0] =  torch.tensor(float('-inf'))\n",
    "            for vi,viv in enumerate(visited_index):\n",
    "                prob[pi+1][viv] =  torch.tensor(float('-inf'))\n",
    "            temp_index = Categorical(F.softmax(prob[pi+1])).sample()\n",
    "            visited_index.append(temp_index)\n",
    "        return torch.tensor(visited_index),F.softmax(prob)\n",
    "\n",
    "        \n",
    "    def policy(self, state):\n",
    "        x = F.relu(self.linear_1(state))\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def get_action(self, state):\n",
    "        prob = self.policy(torch.from_numpy(state).float())\n",
    "        return self.get_action_with_softmax(prob)\n",
    "      \n",
    "    def save(self, item):\n",
    "        self.data.append(item)\n",
    "        \n",
    "    def update(self):\n",
    "        G = 0\n",
    "        self.optimizer.zero_grad()\n",
    "        for reward, prob in self.data[::-1]:\n",
    "            G = reward\n",
    "            loss = torch.tensor(reinforce.get_reward(action), requires_grad=True)\n",
    "            loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.data = []\n",
    "        \n",
    "    def get_reward(self,visit_list):\n",
    "        depot =0\n",
    "        distance = torch.tensor(data['distance_matrix'])[0][visit_list[0]]\n",
    "        prev_cite = visit_list[0]\n",
    "        for i in range(1,14):\n",
    "            distance+= torch.tensor(data['distance_matrix'])[prev_cite][visit_list[i]]\n",
    "            prev_cite =visit_list[i]\n",
    "\n",
    "        distance+= torch.tensor(data['distance_matrix'])[prev_cite][visit_list[14]]\n",
    "        return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c625f75-283c-485d-bb86-3a68f2ba5e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = initialze_value(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82183c3f-c9b2-42fd-96bc-2f6433935182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_for_update(action,prob):\n",
    "    prob_list = torch.zeros((15,15))\n",
    "    for pi,pl in enumerate(prob_list):\n",
    "        prob_list[pi]+= prob[action[pi]]\n",
    "    return prob_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "191a669a-751f-434c-838d-e38997de1ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env):\n",
    "    G = 0.0\n",
    "    for n_epi in range(num_episodes):\n",
    "        state = env['state']\n",
    "        action, prob = agent.get_action(state)\n",
    "        reward = agent.get_reward(action)\n",
    "        agent.save((reward, prob))\n",
    "        G += reward\n",
    "            \n",
    "        agent.update()\n",
    "        if n_epi % print_every==0 and n_epi!=0:\n",
    "            avg_return = G / print_every\n",
    "            print(\"# of episode :{}, avg score : {}\".format(n_epi, avg_return))\n",
    "            G = 0.0\n",
    "            if avg_return <= 650:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ebe6a5e8-f8c5-4a2f-b729-bbaf28e51e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_18648/4081901389.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  first_index = Categorical(F.softmax(prob[0])).sample()\n",
      "C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_18648/4081901389.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  temp_index = Categorical(F.softmax(prob[pi+1])).sample()\n",
      "C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_18648/4081901389.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.tensor(visited_index),F.softmax(prob)\n",
      "C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_18648/2100599326.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(reinforce.get_reward(action), requires_grad=True).backward()\n"
     ]
    }
   ],
   "source": [
    "reinforce = REINFORCE()\n",
    "state =env['state']\n",
    "       \n",
    "action, prob = reinforce.get_action(state)\n",
    "torch.tensor(reinforce.get_reward(action), requires_grad=True).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f07a4b77-907d-4393-8541-3926ba9966e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "zzz = get_prob_for_update(action,prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4dffcfd-835c-4381-94a2-18cf205c0298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_18648/4081901389.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  first_index = Categorical(F.softmax(prob[0])).sample()\n",
      "C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_18648/4081901389.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  temp_index = Categorical(F.softmax(prob[pi+1])).sample()\n",
      "C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_18648/4081901389.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.tensor(visited_index),F.softmax(prob)\n",
      "C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_18648/4081901389.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = torch.tensor(reinforce.get_reward(action), requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of episode :100, avg score : 664.95\n",
      "# of episode :200, avg score : 657.54\n",
      "# of episode :300, avg score : 671.07\n",
      "# of episode :400, avg score : 670.82\n",
      "# of episode :500, avg score : 661.35\n",
      "# of episode :600, avg score : 663.58\n",
      "# of episode :700, avg score : 668.38\n",
      "# of episode :800, avg score : 658.51\n",
      "# of episode :900, avg score : 673.74\n",
      "# of episode :1000, avg score : 661.07\n",
      "# of episode :1100, avg score : 656.98\n",
      "# of episode :1200, avg score : 666.61\n",
      "# of episode :1300, avg score : 666.42\n",
      "# of episode :1400, avg score : 655.55\n",
      "# of episode :1500, avg score : 660.58\n",
      "# of episode :1600, avg score : 656.83\n",
      "# of episode :1700, avg score : 662.92\n",
      "# of episode :1800, avg score : 652.54\n",
      "# of episode :1900, avg score : 665.13\n",
      "# of episode :2000, avg score : 658.27\n",
      "# of episode :2100, avg score : 673.56\n",
      "# of episode :2200, avg score : 662.84\n",
      "# of episode :2300, avg score : 674.42\n",
      "# of episode :2400, avg score : 654.6\n",
      "# of episode :2500, avg score : 669.55\n",
      "# of episode :2600, avg score : 658.85\n",
      "# of episode :2700, avg score : 660.61\n",
      "# of episode :2800, avg score : 670.99\n",
      "# of episode :2900, avg score : 651.95\n",
      "# of episode :3000, avg score : 664.79\n",
      "# of episode :3100, avg score : 660.15\n",
      "# of episode :3200, avg score : 661.06\n",
      "# of episode :3300, avg score : 652.15\n",
      "# of episode :3400, avg score : 659.07\n",
      "# of episode :3500, avg score : 657.63\n",
      "# of episode :3600, avg score : 664.74\n",
      "# of episode :3700, avg score : 648.74\n"
     ]
    }
   ],
   "source": [
    "train(reinforce, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd18993d-071e-421f-83c0-de0312ac382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "zzz =get_prob_for_update(action,prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0058f573-010c-405f-8cd1-4e61ea8f3018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_18648/4081901389.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  first_index = Categorical(F.softmax(prob[0])).sample()\n",
      "C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_18648/4081901389.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  temp_index = Categorical(F.softmax(prob[pi+1])).sample()\n",
      "C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_18648/4081901389.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.tensor(visited_index),F.softmax(prob)\n"
     ]
    }
   ],
   "source": [
    "action, prob = reinforce.get_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eac9fd8f-0438-4b46-87d4-97b647322419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6,  5, 13, 11, 14,  2, 10,  3,  7,  9,  4,  1, 12,  8,  0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c4b43af9-4d99-4bb2-96ae-9eb5cd6b6e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(604., dtype=torch.float64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reinforce.get_reward(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c4aa4d63-c0c8-4c9c-9412-dabc4ba2543a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 5, 13, 11, 14, 2, 10, 3, 7, 9, 4, 1, 12, 8, 0]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[int(a) for a in action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb2fdcfd-aecf-4450-9b4b-366c8e9b02ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(visit_list):\n",
    "    depot =0\n",
    "    distance = (data['distance_matrix'])[0][visit_list[0]]\n",
    "    prev_cite = visit_list[0]\n",
    "    for i in range(1,14):\n",
    "        distance+= (data['distance_matrix'])[prev_cite][visit_list[i]]\n",
    "        prev_cite =visit_list[i]\n",
    "\n",
    "    distance+= (data['distance_matrix'])[prev_cite][visit_list[14]]\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67968592-90f4-4a6a-bef8-931c4bf53652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 1, 14, 8, 4, 6, 2, 11, 13, 9, 7, 5, 3, 10, 0]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af19a914-2d4c-46b9-b4c0-e9420d03a92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "604.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_reward([6, 5, 13, 11, 14, 2, 10, 3, 7, 9, 4, 1, 12, 8, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad6c524-3f48-4628-920f-44fe07490c27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
